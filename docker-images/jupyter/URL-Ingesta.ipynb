{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL, Spark DataFrames, para Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerias Relavantes**\n",
    "\n",
    "`SparkSQL` nos permitirá integrar fuentes de formato no estructurado a datos Tabulares.\n",
    "\n",
    "`BeautifulSoup` nos permitirá de una manera simple hacer WebScraping.\n",
    "\n",
    "`urlparse2` nos permitirá parsear una URL para obtener los datos necesarios para realizar analiis exploratorio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre, empezamos obtniendo el `SparkContext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import urlparse2\n",
    "from urlparse2 import urlparse\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import random\n",
    "#instalar urlopen\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "pd.set_option('display.mpl_style', 'default')\n",
    "get_ipython().magic('matplotlib inline')\n",
    "plt.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerias Relavantes**\n",
    "\n",
    "El Archivo `Link.raw` nos permitirá almacenar los Links Obtenidos por el WebScraping.\n",
    "\n",
    "La primera ejecución el archivo no existira ya que se genera hasta tener el resultado del WebScraping.\n",
    "\n",
    "El commando a continuación nos permite eliminar el archivo, esto si se requiere ejecutar el WebScraping a otra Pagina Objetivo coambiando a la ves el parametro  `urlTarget`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘link.raw’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# define number as a set of words\n",
    "! rm link.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Párametro urlTarget**\n",
    "\n",
    "Cambiar el URL Target cuando se quiera ejecutar el WebScraper para sacar `Link.raw` a donde se requiera.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urlTarget=\"http://www.monster.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retrieves a list of all Internal links found on a page\n",
    "def getInternalLinks(bsObj, includeUrl):\n",
    "    includeUrl = urlparse(includeUrl).scheme+\"://\"+urlparse(includeUrl).netloc\n",
    "    internalLinks = []\n",
    "    #Finds all links that begin with a \"/\"\n",
    "    for link in bsObj.findAll(\"a\", href=re.compile(\"^(/|.*\"+includeUrl+\")\")):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in internalLinks:\n",
    "                if(link.attrs['href'].startswith(\"/\")):\n",
    "                    internalLinks.append(includeUrl+link.attrs['href'])\n",
    "                else:\n",
    "                    internalLinks.append(link.attrs['href'])\n",
    "    return internalLinks\n",
    "\n",
    "#Retrieves a list of all external links found on a page\n",
    "def getExternalLinks(bsObj, excludeUrl):\n",
    "    externalLinks = []\n",
    "    #Finds all links that start with \"http\" or \"www\" that do\n",
    "    #not contain the current URL\n",
    "    for link in bsObj.findAll(\"a\", href=re.compile(\"^(http|www)((?!\"+excludeUrl+\").)*$\")):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in externalLinks:\n",
    "                externalLinks.append(link.attrs['href'])\n",
    "    return externalLinks\n",
    "\n",
    "def getRandomExternalLink(startingPage):\n",
    "    html = urlopen(startingPage)\n",
    "    bsObj = BeautifulSoup(html)\n",
    "    externalLinks = getExternalLinks(bsObj, urlparse(startingPage).netloc)\n",
    "    if len(externalLinks) == 0:\n",
    "        print(\"No external links, looking around the site for one\")\n",
    "        domain = urlparse(startingPage).scheme+\"://\"+urlparse(startingPage).netloc\n",
    "        internalLinks = getInternalLinks(bsObj, domain)\n",
    "        return getRandomExternalLink(internalLinks[random.randint(0,len(internalLinks)-1)])\n",
    "    else:\n",
    "        return externalLinks[random.randint(0, len(externalLinks)-1)]\n",
    "\n",
    "def followExternalOnly(startingSite):\n",
    "    externalLink = getRandomExternalLink(startingSite)\n",
    "    print(\"Random external link is: \"+externalLink)\n",
    "    c = urlparse(externalLink)\n",
    "    with open(\"link.raw\", \"a\") as link_log_c:\n",
    "        link_log_c.write('\"'+startingSite+'\"|\"'+str(c.scheme)+'\"|\"'+str(c.hostname)+'\"|\"'+str(c.port)+'\"|\"'+str(c.path)+'\"|\"'+str(c.username)+'\"|\"'+str(c.params)+'\"\\n')\n",
    "    followExternalOnly(externalLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Collects a list of all external URLs found on the site\n",
    "allExtLinks = set()\n",
    "allIntLinks = set()\n",
    "cont=0\n",
    "def getAllExternalLinks(siteUrl):\n",
    "    html = urlopen(siteUrl)\n",
    "    domain = urlparse(siteUrl).scheme+\"://\"+urlparse(siteUrl).netloc\n",
    "    bsObj = BeautifulSoup(html)\n",
    "    internalLinks = getInternalLinks(bsObj,domain)\n",
    "    externalLinks = getExternalLinks(bsObj,domain)\n",
    "\n",
    "    for link in externalLinks:\n",
    "        if link not in allExtLinks:\n",
    "            allExtLinks.add(link)\n",
    "            print(\"Random external link is: \"+link)\n",
    "            o = urlparse(link)\n",
    "            with open(\"link.raw\", \"a\") as link_log_o:\n",
    "                link_log_o.write('\"'+siteUrl+'\"|\"'+str(o.scheme)+'\"|\"'+str(o.hostname)+'\"|\"'+str(o.port)+'\"|\"'+str(o.path)+'\"|\"'+str(o.username)+'\"|\"'+str(o.params)+'\"\\n')\n",
    "                return link      \n",
    "           \n",
    "    for link in internalLinks:\n",
    "        if link not in allIntLinks:\n",
    "            allIntLinks.add(link)\n",
    "            getAllExternalLinks(link)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "La siguiente secion de codigo ejecuta `WebScraping` nos permitirá almacenar los Links contenidos en la paguina Web objetivo.\n",
    "\n",
    "La primera ejecución puede marcar errores, sin embargo, el error esta controlado \"casi todo!!\", en caso de algún error ejecutar nuevamente hasta llegar a un numero de Links mayor a 100% (**como sigerencia para que la visualización sea adecuada**).`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pages = set()\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "try:\n",
    "    followExternalOnly(urlTarget)\n",
    "    llIntLinks.add(urlTarget)\n",
    "    getAllExternalLinks(urlTarget)\n",
    "    \n",
    "except ConnectionResetError as a:\n",
    "    print(a)\n",
    "except ValueError as b:\n",
    "    print(b)\n",
    "except KeyboardInterrupt as c:\n",
    "    print(c)\n",
    "except NameError as d:\n",
    "    print(d)\n",
    "except ValueError as f:\n",
    "    print(f)\n",
    "except urllib2.HTTPError as err:\n",
    "   if err.code == 404:\n",
    "       print(\"Execption on Scraping process, try again!!\")\n",
    "   else:\n",
    "       raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validamos El Scrapin de Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora leeremos el archivo de Link.raw creado por el Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! wc -l link.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA**: Aqui validamos que se este generando el archivo con los Links Scrapeados, el número adecuado debe ser mayor a 100, esto se logra al rededor de 7 a 10 ejecuciones, **esto dependerá de la configuración pagina que se esta scrapeando**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD desde los datos Crudos (Stage 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos **Stage #2** generamos la extracción de los Links identificados pasando de RDD a SQLContext, esto para presentar la inforamación de forma tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTransaccion(linea):\n",
    "    cells = linea.split('|')\n",
    "    cells[6] = str(cells[6])\n",
    "    return Transaccion(*cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos **Stage #2** reutilizamos funcion desarrollda en la clase DPA para lectura y formate del schema para RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lnk_rdd = sc.textFile(\"link.raw\")\n",
    "Transaccion = Row('siteUrl','scheme','hostname','port','path','username','params')\n",
    "txs = lnk_rdd.map(getTransaccion)\n",
    "txs_df = txs.toDF()\n",
    "txs_df.registerTempTable('Link')\n",
    "pd_tbl_EASentiment = sqlContext.sql('select siteUrl, scheme, hostname, port, path, username, params from Link').toPandas()\n",
    "pd_tbl_EASentiment.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF y SQLContext (Stage 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trabaja con la información ahora en forma tabular para tener los datos **consolidados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de Datos con Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links Relacionados a la Paguina Web Scrapeada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Se extraen los links internos y externos en lapagina Web que se esta **análizando**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_links = sqlContext.sql('select hostname, count(hostname) as total_hostname from Link group by hostname order by total_hostname desc').toPandas()\n",
    "links_by_hostname_pd = num_links.head(15)\n",
    "links_by_hostname_pd=links_by_hostname_pd.set_index(['hostname'])\n",
    "\n",
    "links_by_hostname_pd.plot(kind='barh',fontsize=15)\n",
    "# Etiquetas de la tabla\n",
    "plt.xlabel('Número de Hostname')\n",
    "plt.ylabel('Paginas Web')\n",
    "plt.title('Top 15 de Paginas que tienen Link con Hostname',)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Verificar que poguina tienen mas Links con el Hostname **(Top 15)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "projects_by_sentiment_pd = sqlContext.sql('select hostname, count(hostname) as total_hostname from Link group by hostname order by total_hostname desc').toPandas()\n",
    "projects_by_sentiment_pd = projects_by_sentiment_pd.head(10)\n",
    "projects_by_sentiment_pd=projects_by_sentiment_pd.set_index(['hostname'])\n",
    "# Propiedades de Plot\n",
    "plt.rcParams['figure.figsize'] = (25, 25)\n",
    "projects_by_sentiment_pd.plot(kind='pie',subplots=True,autopct='%.2f',fontsize=15)\n",
    "plt.title('Proporcion de Paginas',fontsize=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Proporcion de Paguinas que tienen realación con el Hostname Scrapeado\n",
    "- Del grafico anterior, se muestran las categorias mas relevante.\n",
    "- Lo anterior tambien marca un dato curioso que es la ** la correlación **, de contenido entre la URL con los Links a otras paguinas  **podemos inferir calidad de contenido**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
